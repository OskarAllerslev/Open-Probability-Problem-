
 Updated version incorporating the two small corrections
\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem}
\usepackage[a4paper,margin=3cm]{geometry}
\usepackage{bm}

\usepackage[utf8]{inputenc}   % allow UTF-8 in the file
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{enumitem}         % for (i), (ii) labels
\usepackage{geometry}         % nicer margins (optional)
\newtheorem{theorem}{Theorem}

\begin{document}

\section*{Problem for bachelors (danish)}
\begin{proof}
	\\ \\
Da $\xi < \frac{1}{3} \Rightarrow E(Z^3)< \infty$ for $Z \sim \text{GPD}$. 
Lyapunov clt anvendes, da vi ikke ønsker at antage at de stokastiske variable er identiske fordelt. Der skal defineres:
	\begin{align*}
		S_n^2 = \sum_{i=1}^n \sigma_i^2
	\end{align*}
Der benyttes to støtteresultater.
Man kan finde et $C$ således at 
	\begin{align*}
		E(|X_i - \mu_i|^3) &\leq E(X_i^3) + 3 E(X_i^2) \mu_i + 3 E(X_i) \mu_i^2 + \mu_i^3 \leq \infty\\
		&\Rightarrow E(|X_i -\mu_i|^3) \leq \underbrace{\sup_i \frac{E(|X_i - \mu_i|^3)}{\sigma_i^3}}_{C}\sigma_i^3 < \infty
	\end{align*}
Derudover 
	\begin{align*}
		\sum_i \sigma_i^3 &= \sum_i \sigma_i^2 \sigma_i \\
		&= \max_j \sigma_j \sum_i \sigma_i^2
	\end{align*}
Så
	\begin{align*}
		\lim_{n \rightarrow \infty} \frac{1}{S_n^{2 + \delta}}\sum_{i=1}^n E\left(\left|X_i - \mu \right|^{2+\delta} \right) &\overset{\delta =1}{=}\lim_{n\rightarrow \infty} \frac{1}{S_n^{3}}\sum_{i=1}^n E|X_i - \mu_i|^3 \\
		&\leq \lim_{n\rightarrow \infty}\frac{C}{S_n^3}\sum_{i=1}^n  \sigma^3_i \\
		&\leq  \lim_{n\rightarrow \infty}\frac{C}{S_n^3}(\max_i \sigma_i ) \sum_{i=1}^n \sigma_i^2\\
		&=\lim_{n\rightarrow \infty}C \frac{\max_i \sigma_i}{S_n} \\
		&= 0
	\end{align*}


\end{proof}

\newpage
\title{Selected problems from Foundations of Modern Probability}
Here i am to show some solutions to selected problems from the book by Kallenberg.

\section{Sets and functions, measures and integration}
\subsection*{Problem 3}
For any space $S$ let $\mu A$ denote the cardinality of a set $A \subset S$. Show that $\mu$ is a measure in $(S, 2^S)$.
\begin{proof}
We apply the definition of a measure.
\textbf{1. Null empty set:}  
We have
\[
\mu(\emptyset) = \#\emptyset = 0.
\]
\textbf{2. Countable additivity:}  
Let \( (A_k)_{k \geq 1} \subset 2^S \) be a countable sequence of pairwise disjoint subsets of \( S \). Then their union is:
\[
\mu\left( \bigcup_{k \geq 1} A_k \right) = \#\left( \bigcup_{k \geq 1} A_k \right).
\]
Since the sets \( A_k \) are disjoint, each element in the union belongs to exactly one \( A_k \), so the cardinality of the union is the sum of the cardinalities:
\[
\#\left( \bigcup_{k \geq 1} A_k \right) = \sum_{k=1}^\infty \#A_k = \sum_{k=1}^\infty \mu(A_k).
\]

Hence, \( \mu \) is countably additive.  \\
\end{proof}


\subsection*{Problem 10 -- Fubini--Tonelli with counting measure}

Let $(S,\mathcal{S},\mu)$ be a $\sigma$--finite measure space and let
\[
\bigl(\mathbb{N},\,2^{\mathbb{N}},\,\nu\bigr), \qquad \nu(A)=\#A,
\]
be the counting--measure space on the natural numbers.
Write $\mu\otimes\nu$ for the product measure on
$\bigl(S\times\mathbb{N},\,\mathcal{S}\otimes 2^{\mathbb{N}}\bigr)$.

\medskip
\textbf{Theorem (Tonelli--Fubini).}\;
Let \(f:S\times\mathbb{N}\to[-\infty,\infty]\) be
\(\mathcal{S}\otimes 2^{\mathbb{N}}\)-measurable.

\begin{enumerate}[label=(\roman*)]
\item (\emph{Tonelli}) If \(f\ge 0\), then
\begin{equation}\label{eq:tonelli}
   \int_{S\times\mathbb{N}} f(s,t)\,(\mu\otimes\nu)(ds,dt)
   \;=\;
   \int_S \Bigl(\sum_{t\in\mathbb{N}} f(s,t)\Bigr)\,\mu(ds)
   \;=\;
   \sum_{t\in\mathbb{N}} \int_S f(s,t)\,\mu(ds),
\end{equation}
allowing the value \(+\infty\).

\item (\emph{Fubini}) If \(f\in L^{1}(\mu\otimes\nu)\),  
all three integrals in \eqref{eq:tonelli} are finite and equal, and both
iterated integrals exist as absolutely convergent expressions.
\end{enumerate}

\begin{proof}
\break
\textbf{Step 1. Indicator rectangles.}\;
Let \(A=B\times C\) with \(B\in\mathcal{S}\) and \(C\subseteq\mathbb{N}\).
For the indicator \(\mathbf{1}_{A}(s,t)=\mathbf{1}_{B}(s)\,\mathbf{1}_{C}(t)\) we have
\[
\int_{S\times\mathbb{N}} \mathbf{1}_{A}\,d(\mu\otimes\nu)
   =(\mu\otimes\nu)(A)=\mu(B)\,\nu(C).
\]
On the other hand,
\[
\int_S \sum_{t\in\mathbb{N}}\mathbf{1}_{A}(s,t)\,\mu(ds)
   =\int_S \mathbf{1}_{B}(s)\Bigl(\sum_{t\in C}1\Bigr)\mu(ds)
   =\mu(B)\,\#C
   =\mu(B)\,\nu(C),
\]
and the same equality holds if the order of integration and summation is
reversed.  Hence \eqref{eq:tonelli} holds for \(\mathbf{1}_{A}\).

\smallskip
\textbf{Step 2. Simple functions.}\;
Any non‑negative simple function can be written
\(f=\sum_{k=1}^{m}c_k\,\mathbf{1}_{A_k}\) with \(c_k\ge 0\) and \(A_k\) rectangles
as above.  By linearity of the integral and Step 1, \eqref{eq:tonelli}
holds for every such \(f\).

\smallskip
\textbf{Step 3. Non‑negative measurable functions.}\;
For a general \(f\ge 0\) choose an increasing sequence of simple
functions \((f_n)_{n\ge 1}\) with \(f_n\uparrow f\).
Applying the Monotone Convergence Theorem on both sides of
\eqref{eq:tonelli} and using Step 2 yields the Tonelli identity for \(f\).

\smallskip
\textbf{Step 4. Integrable functions.}\;
If \(f\in L^{1}(\mu\otimes\nu)\), decompose \(f=f^{+}-f^{-}\) with
\(f^{\pm}\ge 0\) and \(f^{\pm}\in L^{1}\).
Apply Step 3 to \(f^{+}\) and \(f^{-}\) separately and subtract;
finiteness follows from integrability.  Thus \eqref{eq:tonelli} holds
and all integrals are finite.

\smallskip
Steps 1--4 establish Tonelli’s theorem for \(f\ge 0\) and Fubini’s
theorem for \(f\in L^{1}(\mu\otimes\nu)\).
\end{proof}

\section*{Processes, Distributions and independence}

\textbf{Problem 4.} \newline
Let $X,Y \in \{0,1\}$ on an index set $T$. \newline 
Show::
\begin{align}
	X \overset{d}{=} Y \iff P\left(\sum_i x_{t_i} > 0 \right) = P\left(\sum_i y_{t_i} > 0 \right)
\end{align}
\begin{proof}
Let $S=\{0,1\}$ and equip $S^{T}$ with the product $\sigma$–algebra
$\mathcal B$.

\medskip
\noindent\textbf{($\Rightarrow$)}\;
If $X\stackrel d=Y$, then for every $A\in\mathcal B$ we have
$\mathbb P(X\in A)=\mathbb P(Y\in A)$.  
Taking
\(
A=\bigl\{f\in S^{T}:\sum_{i=1}^{n}f_{t_{i}}>0\bigr\}
\)
gives
\[
\mathbb P\!\Bigl(\sum_{i=1}^{n}X_{t_{i}}>0\Bigr)=
\mathbb P\!\Bigl(\sum_{i=1}^{n}Y_{t_{i}}>0\Bigr)
\qquad(\forall n,\;t_{1},\dots ,t_{n}\in T).
\]

\medskip
\noindent\textbf{($\Leftarrow$)}\;
Assume these equalities hold for every finite set of indices.  
Define the class
\[
\mathcal D:=\bigl\{A\in\mathcal B:
      \mathbb P(X\in A)=\mathbb P(Y\in A)\bigr\}.
\]

\emph{Claim: \(\mathcal D\) is a dynkin–system.}
Indeed,
(i) $S^{T}\in\mathcal D$;
(ii) if $A\in\mathcal D$ then
$S^{T}\!\setminus\!A\in\mathcal D$ by complementing the equal
probabilities;  
(iii) if $(A_k)_{k\ge1}\subset\mathcal D$ are disjoint, then
countable additivity gives
\(\mathbb P(X\in\bigcup_kA_k)=\mathbb P(Y\in\bigcup_kA_k)\).
Hence \(\mathcal D\) is a Dynkin-system.

Next, for every finite non–empty
\(J=\{t_{1},\dots ,t_{m}\}\subset T\) set
\[
Z_{J}:=\bigl\{f\in S^{T}:f_{t_{i}}=0\ \forall i\in J\bigr\}.
\]
Because $Z_{J}$ is the complement of the event
\(\{\sum_{i=1}^{m}f_{t_{i}}>0\}\),
our hypothesis puts \(Z_{J}\) in \(\mathcal D\).
Let
\(
\mathcal C:=\{Z_{J}:0<|J|<\infty\}.
\)

\emph{Claim: \(\mathcal C\) is a pi–system.}
For finite $J,K\subset T$,
\(Z_{J}\cap Z_{K}=Z_{J\cup K}\), so finite intersections stay inside
\(\mathcal C\).

Thus we have a pi–system \(\mathcal C\subset\mathcal D\).
By the π–λ (Dynkin–Sierpiński) theorem,
\[
\sigma(\mathcal C)\subset\mathcal D.
\]

\medskip
\emph{But \(\sigma(\mathcal C)=\mathcal B\).}
For any $t\in T$ the single‑coordinate event
\(\{f_{t}=1\}\) is \(Z_{\{t\}}^{\mathrm c}\in\sigma(\mathcal C)\).
Finite intersections of such events and their complements therefore
yield every cylinder set of the form
\(\{f:(f_{t_{1}},\dots ,f_{t_{n}})\in B\}\),
and the cylinders generate the product σ‑algebra.  

\medskip
Consequently \(\mathcal B\subset\mathcal D\); i.e.
\(\mathbb P(X\in A)=\mathbb P(Y\in A)\) for all
\(A\in\mathcal B\).  That is exactly \(X\stackrel d=Y\).
\end{proof}

\textbf{Problem 10.}
\newline
Let $\xi_i$ i.i.d on $(S, \mathcal{S}, \mu)$ and $\tau = \{ k \geq 1 : \xi_k \in A \}$. Fix $A \in \mathcal{S}$ and set $\mu(A)>0$. 
\newline 
Show that the distribution of $\xi_\tau$ is $\mu(\cdot \mid A)$. 
\newline 
\begin{proof}
\newline
Start by noting that 
	\begin{align}
		\{\xi_\tau \in B \} &= \bigcup_{k=1}^\infty \{\tau = k \cap \xi_k \in B\}
	\end{align}
Next we note that the sets are disjoint, and so we use the property of a measure. 
	\begin{align}
		P(\xi_\tau \in B ) &= \sum_{k=1}^\infty P(\tau = k \cap \xi_k \in B) \\
		&= \sum_{K=1}^\infty P(\xi_1 \notin A, \cdots, \xi_k \in A \cap B) \\
		&= \sum_{k=1}^\infty \left( \prod_{i=1}^{k-1} P(\xi_i \notin A) \right) P(\xi_k \in A \cap B)\\
		&= \sum_{k=1}^\infty (1- \mu(A))^{k-1} \mu(A\cap B) \\
		&= \mu(A)^{-1}\mu(B \cap A)
	\end{align}
	Holds $\forall B \subset \mathcal{S}$.

\end{proof}

\textbf{Problem 11.}
\newline 
Let $\xi_i \in [0,1]$ i.i.d. Show that $E\left(\prod_n \xi_n\right) = \prod_n E(\xi_n)$.
\newline
\begin{proof}
\newline
	let $X_i: ([0,1], \mathcal{B}([0,1]), P) \rightarrow \mathbb{R}$ be $\mathcal{L}^1$.
	Define $\mu_k = P \circ X_k^{-1}$ and $\mu = \bigotimes_n \mu_n$, be independent.
	\begin{align}
		E\left( \prod_{k=1}^n X_k\right) &= \int_\Omega \prod_{k=1}^n X_k(\omega)dP(\omega) \\
		&\overset{Indp.}{=} \int_{[0,1]^n} \prod_{k=1}^n x_k d(\mu_1 \otimes \cdots \otimes \mu_n) \\
		&\overset{FB}{=} \int_{[0,1]} x_n d\mu_n \int_{[0,1]^{n-1}} \prod_{k=1}^{n-1} x_k d(\mu_1 \otimes \cdots \otimes \mu_{n-1}) \\
		&\overset{iter.}{=} \prod_{k=1}^n \int_{[0,1]} x_k d\mu_k
	\end{align}
\newline
	The proof can be extended to apply for probabilities by setting $X_k(\omega) = 1\{A_k(\omega)\}$.
\end{proof}

\textbf{Problem 3.}
\newline
We are $F$ which is right cont. with bounded variation and is zero at minus infinity. We are asked to show that $E(F(\xi)) = \int P(\xi \geq t) dF(t)$. 
\begin{proof}
Define the push forward / law of $\xi$ as $\mu_\xi = P \circ \xi^{-1}$ or rather $\mu_\xi(B) = P(\xi \in B)$.

\begin{align}
	E[F(\xi)] &= \int_\Omega \biggl(\int_{\mathbb{R}} 1_{\{s\ge t\}} \,dF(t)\biggr)\,d\mu_\xi(s) \\
	&\overset{\text{FT}}{=} \int_{\mathbb{R}} \biggl(\int_\Omega 1_{\{s\ge t\}} \,d\mu_\xi(s)\biggr)\,dF(t) \\
	&= \int_{\mathbb{R}} \mu_\xi\bigl([t,\infty)\bigr)\,dF(t) \\
	&= \int_{\mathbb{R}} P(\xi \ge t)\,dF(t).
\end{align}
\end{proof}


\newpage 
\section*{Conditioning and disintegration}

\subsection*{Problem 11}
Show $E^{\mathcal{F} \vee 1_{A}}\xi = \frac{E^\mathcal{F}\xi;A}{P^\mathcal{F}A}$
\\
\begin{proof}
\begin{align*}
	E^\mathcal{F} \xi ; A &= E^\mathcal{F} E^{\mathcal{F} \vee 1_{A}}\xi 1_{A}\\
	&= E^\mathcal{F}1_A E^{\mathcal{F} \vee 1_{A}}\xi 
\end{align*}
which implies 
\begin{align*}
	\frac{E^\mathcal{F} \xi ; A}{P^\mathcal{F}A}&=  E^{\mathcal{F} \vee 1_{A}}\xi 
\end{align*}
\end{proof}

\newpage
\section*{Optimal times and martingales}
\subsection*{Problem 17}
Let $\xi_n \rightarrow \xi$ in $L^1$ and let $\mathcal{F}_n$ be an increasing filtration.\\ 
\\
Show: 
\begin{align*}
	E(\xi_n \mid \mathcal{F}_n) \overset{L^1}{\rightarrow} E(\xi \mid \mathcal{F}_\infty)
\end{align*}
\begin{proof}
	\\ \\
	\begin{align*}
		|| E(\xi_n \mid \mathcal{F}_n) - E(\xi \mid \mathcal{F}_\infty) || &\leq || E(\xi_n \mid \mathcal{F}_n) - E(\xi \mid \mathcal{F}_n)|| + || E(\xi \mid \mathcal{F}_n)- E(\xi \mid \mathcal{F}_\infty) || \\ 
		&= \underbrace{|| E(\xi_n - \xi \mid \mathcal{F}_n) ||}_{\xi_n \overset{L^1}{\rightarrow} \xi \Rightarrow =0 } + || E(\xi \mid \mathcal{F}_n ) - E(\xi \mid \mathcal{F}_\infty)||\\
		&\overset{th 9.24}{=}  0
	\end{align*}
Where we have used conditioning limits, Jessen, Levy, since we have a filtration on an unboubounded index set, and an $\xi\in L^1$.
\end{proof}

\subsection*{Problem 21}
Let $\tau > 0$ and let the filtration $\mathcal{F}=(\mathcal{F}_t)$ be right cont. \\ \\
Show that $X_t = P(\tau < t \mid \mathcal{F}_t)$ has a right cont. version $(X_t = X_{t+}) \hspace{0.5cm} \forall t \geq 0$
\begin{proof}\\ \\
	\begin{align*}
		E(X_t \mid \mathcal{F}_s) &= E(P(\tau < t \mid \mathcal{F}_t ) \mid \mathcal{F}_s) \\
		&=P(\tau < t \mid \mathcal{F}_s) \\
		&=P(\tau < t \mid ) \\
		&\geq X_s
	\end{align*}
	Hence $X_t$ is a sub-martingale. We also note that $EX$ is right cont. since 
	\begin{align*}
		E(X_t) &= E(P(\tau < t \mid \mathcal{F}_t)) \\
		&= P(\tau < t) 
	\end{align*}

	Then we apply theorem 9.28 (regularization, Doob) (ii) which states that for any $\mathcal{F}$ sub-martingale X on $\mathbb{R}_+$ with restriction $Y$ to $\mathbb{Q}_+$ we have 
	that when $\mathcal{F}$ is right cont., $X$ has a right cont. version with left hand limits iff $EX$ is right cont. 
\end{proof}


\end{document}




























































